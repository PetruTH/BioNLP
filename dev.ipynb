{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "client_id = 'CLIENT_ID'\n",
    "client_secret = 'CLIENT_SECRET'\n",
    "user_agent = 'scraper_demo_v1'\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=client_secret,\n",
    "                     user_agent=user_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aeb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f14344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "query = (\"depresie OR anxietate OR panica OR plans OR suferinta OR singuratate OR dezamagire\"\n",
    "         \" OR frica OR \\\"probleme in familie\\\" OR \\\"ganduri negre\\\" OR PTSD\"\n",
    "         \"OR tristete OR sinucidere OR suicidal OR angoasa\")\n",
    "\n",
    "subreddits = [\n",
    "    \"Roumanie\", \"moldova\", \n",
    "    'romania', 'CasualRO', \"WomenRO\", \"Men_RO\", \n",
    "    \"bucuresti\", \"brasov\", \"cluj\", \"iasi\"\n",
    "]\n",
    "\n",
    "emotion_keywords = [\n",
    "    'depresie', 'anxietate', 'panica', 'plans', 'suferinta', 'singuratate', 'dezamagire',\n",
    "    'frica', 'probleme', 'familie', 'ganduri', 'negre', 'trist', 'oboseala', 'insomnie',\n",
    "    'gol', 'inutil', 'vinovatie', 'neputinta', 'lipsa', 'speranta', 'durere', 'PTSD',\n",
    "    'sinucidere', 'suicidal', 'angoasa', 'tristete'\n",
    "]\n",
    "\n",
    "def contains_emotion_keyword(sentence):\n",
    "    return any(keyword.lower() in sentence.lower() for keyword in emotion_keywords)\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    search_results = reddit.subreddit(subreddit).search(query, limit=1500)\n",
    "\n",
    "    posts = []\n",
    "    seen_sentences = set()\n",
    "\n",
    "    for post in tqdm(search_results):\n",
    "        if len(post.selftext.split()) > 9:\n",
    "            sentences = re.split(r'[.!?\\n]+(?:\\s|$)', post.selftext)\n",
    "            for sentence in sentences:\n",
    "                cleaned_sentence = sentence.strip()\n",
    "                if len(cleaned_sentence.split()) >= 3:\n",
    "                    if cleaned_sentence not in seen_sentences:\n",
    "                        if contains_emotion_keyword(cleaned_sentence):\n",
    "                            posts.append([cleaned_sentence, post.title, post.subreddit.display_name, 'post'])\n",
    "                            seen_sentences.add(cleaned_sentence)\n",
    "\n",
    "        post.comment_sort = 'top'    \n",
    "        post.comments.replace_more(limit=0)\n",
    "        top_comments = post.comments[:15]   \n",
    "\n",
    "        for comment in top_comments:\n",
    "            comment_body = comment.body\n",
    "\n",
    "            if len(comment_body.split()) > 9:\n",
    "                sentences = re.split(r'[.!?\\n]+(?:\\s|$)', comment_body)\n",
    "                for sentence in sentences:\n",
    "                    cleaned_sentence = sentence.strip()\n",
    "                    if len(cleaned_sentence.split()) >= 3:\n",
    "                        if cleaned_sentence not in seen_sentences:\n",
    "                            if contains_emotion_keyword(cleaned_sentence):\n",
    "                                posts.append([cleaned_sentence, post.title, post.subreddit.display_name, 'comment'])\n",
    "                                seen_sentences.add(cleaned_sentence)\n",
    "\n",
    "    df = pd.DataFrame(posts, columns=['sentence', 'post_title', 'subreddit', 'source'])\n",
    "    df.index = range(1, len(df) + 1)\n",
    "    df.to_csv(f'secondVersion\\\\output_sentences_top10_comments_{subreddit}.csv', index_label='Index')\n",
    "\n",
    "    print(f\"Am salvat {len(posts)} propoziții filtrate (postări + top 10 comentarii) din subredditul: {subreddit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "directory = 'secondVersion'\n",
    "pattern = os.path.join(directory, 'output_sentences_top10*.csv')\n",
    "\n",
    "file_list = glob.glob(pattern)\n",
    "\n",
    "df_list = [pd.read_csv(file) for file in file_list]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "merged_df.to_csv('merged_output_sentences_top10.csv', index_label='Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2cc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "df = pd.read_csv('merged_output_sentences_top10.csv')\n",
    "\n",
    "n = len(df)\n",
    "split_size = math.ceil(n / 3)\n",
    "\n",
    "for i in range(3):\n",
    "    start = i * split_size\n",
    "    end = min((i + 1) * split_size, n)\n",
    "    sub_df = df.iloc[start:end].reset_index(drop=True)\n",
    "    sub_df.to_csv(f'merged_output_sentences_top10_part{i+1}.csv', index_label='Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-DEPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "query = (\"fericire OR bucurie OR optimism OR zambet OR implinire OR concediu OR calatorie \"\n",
    "         \"OR mancare OR dragut OR relaxare OR succes OR realizare OR distractie OR veselie \"\n",
    "         \"OR satisfactie OR multumire OR sanatate OR entuziasm OR hobby OR sport OR prietenie \"\n",
    "         \"OR iubire OR apreciere OR cadou OR sarbatoare OR petrecere OR vacanta OR familie \"\n",
    "         \"OR amuzant OR comedie OR surpriza OR jocuri OR pasiune OR weekend OR placere \"\n",
    "         \"OR liniste OR echilibru OR energie OR frumos OR natura OR aventura OR plimbare\")\n",
    "\n",
    "subreddits = [\n",
    "    \"Roumanie\", \"moldova\", \n",
    "    'romania', 'CasualRO', \"WomenRO\", \"Men_RO\", \n",
    "    \"bucuresti\", \"brasov\", \"cluj\", \"iasi\"\n",
    "]\n",
    "\n",
    "emotion_keywords = [\n",
    "    'fericire', 'liniste', 'bucurie', 'optimism', 'viata', 'frumoasa', 'zambet',\n",
    "    'simpla', 'buna', 'linistita', 'zambete', 'multumire', 'implinire', 'concediu',\n",
    "    'calatorie', 'mancare', 'dragut', 'relaxare', 'succes', 'realizare',\n",
    "    'distractie', 'veselie', 'satisfactie', 'sanatate', 'entuziasm' 'amuzant',\n",
    "    'comedie', 'surpriza', 'jocuri', 'pasiune', 'weekend', 'placere', 'echilibru',\n",
    "    'energie', 'frumos', 'natura', 'aventura', 'plimbare', 'prieteni', 'iubire',\n",
    "    'apreciere', 'cadou', 'sarbatoare', 'petrecere', 'vacanta', 'familie', 'hobby', 'sport'\n",
    "]\n",
    "\n",
    "ban_words = [\n",
    "    'depresie', 'anxietate', 'panica', 'plâns', 'suferință', 'singurătate', 'dezamăgire',\n",
    "    'frică', 'moarte', 'suicid', 'sinucidere',\n",
    "    'sex', 'drog', 'iarba', 'alcool'\n",
    "]\n",
    "\n",
    "def contains_emotion_keyword(sentence):\n",
    "    return any(keyword.lower() in sentence.lower() for keyword in emotion_keywords)\n",
    "\n",
    "def contains_ban_word(sentence):\n",
    "    return any(ban_word.lower() in sentence.lower() for ban_word in ban_words)\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    search_results = reddit.subreddit(subreddit).search(query, limit=1500)\n",
    "\n",
    "    posts = []\n",
    "    seen_sentences = set()\n",
    "\n",
    "    for post in tqdm(search_results):\n",
    "        if len(post.selftext.split()) > 9:\n",
    "            sentences = re.split(r'[.!?\\n]+(?:\\s|$)', post.selftext)\n",
    "            for sentence in sentences:\n",
    "                cleaned_sentence = sentence.strip()\n",
    "                if len(cleaned_sentence.split()) >= 3:\n",
    "                    if cleaned_sentence not in seen_sentences:\n",
    "                        if contains_emotion_keyword(cleaned_sentence) and not contains_ban_word(cleaned_sentence):\n",
    "                            posts.append([cleaned_sentence, post.title, post.subreddit.display_name, 'post'])\n",
    "                            seen_sentences.add(cleaned_sentence)\n",
    "\n",
    "        post.comment_sort = 'top'\n",
    "        post.comments.replace_more(limit=0)\n",
    "        top_comments = post.comments[:15]\n",
    "\n",
    "        for comment in top_comments:\n",
    "            comment_body = comment.body\n",
    "            if len(comment_body.split()) > 2:\n",
    "                sentences = re.split(r'[.!?\\n]+(?:\\s|$)', comment_body)\n",
    "                for sentence in sentences:\n",
    "                    cleaned_sentence = sentence.strip()\n",
    "                    if len(cleaned_sentence.split()) >= 3:\n",
    "                        if cleaned_sentence not in seen_sentences:\n",
    "                            if contains_emotion_keyword(cleaned_sentence) and not contains_ban_word(cleaned_sentence):\n",
    "                                posts.append([cleaned_sentence, post.title, post.subreddit.display_name, 'comment'])\n",
    "                                seen_sentences.add(cleaned_sentence)\n",
    "\n",
    "    df = pd.DataFrame(posts, columns=['sentence', 'post_title', 'subreddit', 'source'])\n",
    "    df.index = range(1, len(df) + 1)\n",
    "    df.to_csv(f'secondVersionNonDepression\\\\output_sentences_top10_comments_{subreddit}.csv', index_label='Index')\n",
    "\n",
    "    print(f\"Am salvat {len(posts)} propoziții curate (pozitive) din subredditul: {subreddit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "directory = 'secondVersionNonDepression'\n",
    "pattern = os.path.join(directory, 'output_sentences_top10*.csv')\n",
    "\n",
    "file_list = glob.glob(pattern)\n",
    "\n",
    "df_list = [pd.read_csv(file) for file in file_list]\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "merged_df.to_csv('merged_output_sentences_NON_DEPRESSION.csv', index_label='Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0772c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'reddit_depression'  \n",
    "file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "print(f\"Number of files in '{folder_path}':\", file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'reddit_non_depression'  \n",
    "file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "print(f\"Number of files in '{folder_path}':\", file_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
